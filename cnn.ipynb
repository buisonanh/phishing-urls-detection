{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  http://1337x.to/torrent/1048648/American-Sniper-2014-MD-iTALiAN-DVDSCR-X264-BST-MT/\n",
      "0  http://1337x.to/torrent/1110018/Blackhat-2015-...                                 \n",
      "1  http://1337x.to/torrent/1122940/Blackhat-2015-...                                 \n",
      "  http://v2.email-marketing.adminsimple.com/track/link?s=a879370e133bf6f71b5cc7ce0c2043e1&amp;AdministratorID=20238&amp;MemberID=21410&amp;CampaignID=1&amp;CampaignStatisticsID=1&amp;URL=http%3A%2F%2Fwww.cadivi.pro%2Faprobados%2Faprobados2012.php%3Futm_source%3Dv2.email-marketing.adminsimple.com\n",
      "0  http://bid.openx.net/json?amp;amp;amp;amp;cid;...                                                                                                                                                                                                                                                    \n",
      "1  http://webmail2.centurytel.net/hwebmail/servic...                                                                                                                                                                                                                                                    \n"
     ]
    }
   ],
   "source": [
    "# Import data\n",
    "legit = pd.read_csv(\"datasets/unb-university/Benign_list_big_final.csv\")\n",
    "phishing = pd.read_csv(\"datasets/unb-university/phishing_dataset.csv\")\n",
    "\n",
    "print(legit.head(2))\n",
    "print(phishing.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "legit.columns = ['url']\n",
    "phishing.columns = ['url']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Preparing Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Legit set null values: url    0\n",
      "dtype: int64\n",
      "\n",
      "Phishing set null values: url    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check null values\n",
    "print(f\"Legit set null values: {legit.isnull().sum()}\")\n",
    "print()\n",
    "print(f\"Phishing set null values: {phishing.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Legit's duplicates num: 0\n",
      "Phishing duplicates num: 9\n",
      "\n",
      "After dropping duplicates\n",
      "Legit's duplicates num: 0\n",
      "Phishing duplicates num: 0\n"
     ]
    }
   ],
   "source": [
    "# Drop duplicates\n",
    "print(f\"Legit's duplicates num: {legit.duplicated().sum()}\")\n",
    "print(f\"Phishing duplicates num: {phishing.duplicated().sum()}\")\n",
    "\n",
    "legit = legit.drop_duplicates()\n",
    "phishing = phishing.drop_duplicates()\n",
    "print()\n",
    "print(\"After dropping duplicates\")\n",
    "\n",
    "print(f\"Legit's duplicates num: {legit.duplicated().sum()}\")\n",
    "print(f\"Phishing duplicates num: {phishing.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35377, 1)\n",
      "(9955, 1)\n"
     ]
    }
   ],
   "source": [
    "# Check if there is imbalance\n",
    "print(legit.shape)\n",
    "print(phishing.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25422, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the right amount of phishing urls data from phishtank\n",
    "phishtank = pd.read_csv('datasets/phishtank-phishing-urls.csv')\n",
    "\n",
    "# Get the missing amount\n",
    "legit_mal_diff = int(len(legit.index) - len(phishing.index))\n",
    "\n",
    "# Randomly select the urls from phishtank dataset\n",
    "phishtank = phishtank.sample(n=legit_mal_diff,random_state=42)\n",
    "phishtank.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35377, 1)\n",
      "(35377, 1)\n"
     ]
    }
   ],
   "source": [
    "phishing_added = pd.concat([phishing, phishtank])\n",
    "\n",
    "print(legit.shape)\n",
    "print(phishing_added.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a 'label' column to the dataframes\n",
    "legit[\"label\"] = 0\n",
    "phishing_added[\"label\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70754, 2)\n"
     ]
    }
   ],
   "source": [
    "# combine the two dataframes\n",
    "df = pd.concat([legit, phishing_added], ignore_index=True)\n",
    "\n",
    "# shuffle the dataframe's rows randomly\n",
    "df = df.sample(frac=1, random_state=42) # Set random_state to an integer for reproducibility\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url      0\n",
      "label    0\n",
      "dtype: int64\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())\n",
    "print(df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Transform Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Extract features (URLs) and labels\n",
    "X = df['url'].values\n",
    "y = df['label'].values\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'<unk>'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 37\u001b[0m\n\u001b[0;32m     34\u001b[0m max_sequence_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(tokens) \u001b[38;5;28;01mfor\u001b[39;00m tokens \u001b[38;5;129;01min\u001b[39;00m X_train_tokens \u001b[38;5;241m+\u001b[39m X_test_tokens)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Convert tokens to numerical indices with padding, using <unk> for unknown tokens\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m unk_index \u001b[38;5;241m=\u001b[39m \u001b[43mvocab\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m<unk>\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     39\u001b[0m X_train_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mLongTensor([\n\u001b[0;32m     40\u001b[0m     [vocab\u001b[38;5;241m.\u001b[39mget(token, unk_index) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens] \u001b[38;5;241m+\u001b[39m [\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m (max_sequence_length \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokens))\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m tokens \u001b[38;5;129;01min\u001b[39;00m X_train_tokens\n\u001b[0;32m     42\u001b[0m ])\n\u001b[0;32m     44\u001b[0m X_test_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mLongTensor([\n\u001b[0;32m     45\u001b[0m     [vocab\u001b[38;5;241m.\u001b[39mget(token, unk_index) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens] \u001b[38;5;241m+\u001b[39m [\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m (max_sequence_length \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokens))\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m tokens \u001b[38;5;129;01min\u001b[39;00m X_test_tokens\n\u001b[0;32m     47\u001b[0m ])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchtext\\vocab\\vocab.py:65\u001b[0m, in \u001b[0;36mVocab.__getitem__\u001b[1;34m(self, token)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mexport\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, token: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m     58\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03m        token: The token used to lookup the corresponding index.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;124;03m        The index corresponding to the associated token.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mKeyError\u001b[0m: '<unk>'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchtext.vocab import Vocab\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from collections import Counter  # Corrected import\n",
    "\n",
    "# Download the punkt tokenizer (needed for word_tokenize)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Tokenize using nltk\n",
    "def tokenize_text(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "\n",
    "# Tokenize URLs\n",
    "X_train_tokens = [tokenize_text(url) for url in X_train]\n",
    "X_test_tokens = [tokenize_text(url) for url in X_test]\n",
    "\n",
    "# Build vocabulary\n",
    "flattened_tokens = [token for sublist in X_train_tokens for token in sublist]\n",
    "counter = Counter(flattened_tokens)\n",
    "\n",
    "# Filter out tokens that occur less than min_freq times\n",
    "min_freq = 1\n",
    "filtered_counter = {word: freq for word, freq in counter.items() if freq >= min_freq}\n",
    "\n",
    "# Create Vocab\n",
    "vocab = Vocab(filtered_counter)\n",
    "\n",
    "# Find the maximum sequence length\n",
    "max_sequence_length = max(len(tokens) for tokens in X_train_tokens + X_test_tokens)\n",
    "\n",
    "# Convert tokens to numerical indices with padding, using <unk> for unknown tokens\n",
    "unk_index = vocab['<unk>']\n",
    "\n",
    "X_train_indices = torch.LongTensor([\n",
    "    [vocab.get(token, unk_index) for token in tokens] + [0] * (max_sequence_length - len(tokens))\n",
    "    for tokens in X_train_tokens\n",
    "])\n",
    "\n",
    "X_test_indices = torch.LongTensor([\n",
    "    [vocab.get(token, unk_index) for token in tokens] + [0] * (max_sequence_length - len(tokens))\n",
    "    for tokens in X_test_tokens\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Padding sequences\n",
    "max_sequence_length = 100\n",
    "X_train_padded = torch.nn.functional.pad(X_train_indices, (0, max_sequence_length - X_train_indices.size(1)))\n",
    "X_test_padded = torch.nn.functional.pad(X_test_indices, (0, max_sequence_length - X_test_indices.size(1)))\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(X_train_padded, torch.FloatTensor(y_train))\n",
    "test_dataset = TensorDataset(X_test_padded, torch.FloatTensor(y_test))\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.6865\n",
      "Epoch [2/10], Loss: 0.6907\n",
      "Epoch [3/10], Loss: 0.7028\n",
      "Epoch [4/10], Loss: 0.7212\n",
      "Epoch [5/10], Loss: 0.6932\n",
      "Epoch [6/10], Loss: 0.6922\n",
      "Epoch [7/10], Loss: 0.6933\n",
      "Epoch [8/10], Loss: 0.6939\n",
      "Epoch [9/10], Loss: 0.7008\n",
      "Epoch [10/10], Loss: 0.6930\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=embedding_dim, out_channels=n_filters, kernel_size=fs)\n",
    "            for fs in filter_sizes\n",
    "        ])\n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        embedded = embedded.permute(0, 2, 1)\n",
    "        conved = [nn.functional.relu(conv(embedded)) for conv in self.convs]\n",
    "        pooled = [nn.functional.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        cat = self.dropout(torch.cat(pooled, dim=1))\n",
    "        return self.fc(cat)\n",
    "\n",
    "# Define model hyperparameters\n",
    "vocab_size = len(TEXT.vocab)\n",
    "embedding_dim = 50\n",
    "n_filters = 128\n",
    "filter_sizes = [3, 4, 5]\n",
    "output_dim = 1\n",
    "dropout = 0.5\n",
    "\n",
    "# Create an instance of the model\n",
    "model = CNNModel(vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.5000\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 10\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch in train_iterator:\n",
    "        text, text_lengths = batch.url\n",
    "        labels = batch.label\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(text).squeeze(1)\n",
    "        loss = criterion(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in test_iterator:\n",
    "        text, text_lengths = batch.url\n",
    "        labels = batch.label\n",
    "        predictions = model(text).squeeze(1)\n",
    "        # Evaluate and report metrics as needed\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
